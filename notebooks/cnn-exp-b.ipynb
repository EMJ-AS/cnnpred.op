{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "CNNPred is code that is based on the paper [CNNPred:CNN-based stock market prediction using several data sources](https://arxiv.org/pdf/1810.08923.pdf). The code implementents two versions which are CNNPred2 and CNNPred3 which use 2D and 3D input datasets.  The 2D data is time history and features for a particular time series. The 3D data is time history and features for multiple time series.  Both version produce a prediction for a particular time series which could be a market index or the price of an equity.  This is the [blog post](https://machinelearningmastery.com/using-cnn-for-financial-time-series-prediction/) that discusses the implmentation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "The following table shows the features used in CNNPred2 and CNNPred3. Note that there is a lot of economic vartiable used from FRED which we could also use. [TA-Lib](https://ta-lib.org/) is the technical indicators library they use and is one of the more popular ones boasting perhaps the largest number of features. [pandas-ta](https://github.com/twopirllc/pandas-ta) is another example and is a newer version that is designed around using pandas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/features-1of2.jpg \"Features used in CNNPred 1 of 2\")\n",
    "\n",
    "![alt text](./img/features-2of2.jpg \"Features used in CNNPred 2 of 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running on a mac, make sure that you have the latest version of the OS, Xcode, and pip.\n",
    "# You will probably need to install a fortran compiler for the compilation of scipy and utilities like\n",
    "# cmake.  Install these using brew.\n",
    "\n",
    "# %pip install tensorflow-macos\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"./data-2d\"\n",
    "TRAIN_TEST_CUTOFF = '2022-05-10'\n",
    "TRAIN_VALID_RATIO = 0.75\n",
    "\n",
    "# DATADIR = \"./Dataset\"\n",
    "# TRAIN_TEST_CUTOFF = '2023-05-21'\n",
    "# TRAIN_VALID_RATIO = 0.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "These are the evaluation metrics that are designed to be used with batches of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "# to implement F1 score for validation in a batch\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = f1_m(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNPred3\n",
    "\n",
    "This is the implementation of CNNPred3 which uses features and multiple equities as time series as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnpred_3d(seq_len=60, n_stocks=5, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"3D-CNNpred model according to the paper\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(n_stocks, seq_len, n_features)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1,1), activation=\"relu\", data_format=\"channels_last\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(n_stocks,3), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(1,2)),\n",
    "        Conv2D(n_filters[2], kernel_size=(1,3), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(1,2)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datagen(data, seq_len, batch_size, target_index, targetcol, kind):\n",
    "    \"As a generator to produce samples for Keras model\"\n",
    "    # Learn about the data's features and time axis\n",
    "    input_cols = [c for c in data.columns if c[0] != targetcol]\n",
    "    tickers = sorted(set(c for _,c in input_cols))\n",
    "    n_features = len(input_cols) // len(tickers)\n",
    "    index = data.index[data.index < TRAIN_TEST_CUTOFF]\n",
    "    split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "    assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
    "    if kind == \"train\":\n",
    "        index = index[:split]   # range for the training set\n",
    "    elif kind == 'valid':\n",
    "        index = index[split:]   # range for the validation set\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    # Infinite loop to generate a batch\n",
    "    batch = []\n",
    "    while True:\n",
    "        # Pick one position, then clip a sequence length\n",
    "        while True:\n",
    "            t = random.choice(index)\n",
    "            n = (data.index == t).argmax()\n",
    "            if n-seq_len+1 < 0:\n",
    "                continue # this sample is not enough for one sequence length\n",
    "            frame = data.iloc[n-seq_len+1:n+1][input_cols]\n",
    "            # convert frame with two level of indices into 3D array\n",
    "            shape = (len(tickers), len(frame), n_features)\n",
    "            X = np.full(shape, np.nan)\n",
    "            for i,ticker in enumerate(tickers):\n",
    "                X[i] = frame.xs(ticker, axis=1, level=1).values\n",
    "            batch.append([X, data[targetcol][target_index][t]])\n",
    "            break\n",
    "        # if we get enough for a batch, dispatch\n",
    "        if len(batch) == batch_size:\n",
    "            X, y = zip(*batch)\n",
    "            yield np.array(X), np.array(y)\n",
    "            batch = []\n",
    "\n",
    "def testgen(data, seq_len, target_index, targetcol):\n",
    "    \"Return array of all test samples\"\n",
    "    input_cols = [c for c in data.columns if c[0] != targetcol]\n",
    "    tickers = sorted(set(c for _,c in input_cols))\n",
    "    n_features = len(input_cols) // len(tickers)\n",
    "    t = data.index[data.index >= TRAIN_TEST_CUTOFF][0]\n",
    "    n = (data.index == t).argmax()\n",
    "    batch = []\n",
    "    for i in range(n+1, len(data)+1):\n",
    "        # Clip a window of seq_len ends at row position i-1\n",
    "        frame = data.iloc[i-seq_len:i]\n",
    "        target = frame[targetcol][target_index][-1]\n",
    "        frame = frame[input_cols]\n",
    "        # convert frame with two level of indices into 3D array\n",
    "        shape = (len(tickers), len(frame), n_features)\n",
    "        X = np.full(shape, np.nan)\n",
    "        for i,ticker in enumerate(tickers):\n",
    "            X[i] = frame.xs(ticker, axis=1, level=1).values\n",
    "        batch.append([X, target])\n",
    "    X, y = zip(*batch)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Read data into pandas DataFrames\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    data = {}\n",
    "    for filename in os.listdir(DATADIR):\n",
    "        if not filename.lower().endswith(\".csv\"):\n",
    "            continue # read only the CSV files\n",
    "        filepath = os.path.join(DATADIR, filename)\n",
    "        X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
    "        # basic preprocessing: get the name, the classification\n",
    "        # Save the target variable as a column in dataframe for easier dropna()\n",
    "        name = X[\"Name\"][0]\n",
    "        del X[\"Name\"]\n",
    "        cols = X.columns\n",
    "        X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
    "        X.dropna(inplace=True)\n",
    "        # Fit the standard scaler using the training dataset\n",
    "        index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
    "        index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "        scaler = StandardScaler().fit(X.loc[index, cols])\n",
    "        # Save scale transformed dataframe\n",
    "        X[cols] = scaler.transform(X[cols])\n",
    "        data[name] = X\n",
    "\n",
    "    # Transform data into 3D dataframe (multilevel columns)\n",
    "    for key, df in data.items():\n",
    "        df.columns = pd.MultiIndex.from_product([df.columns, [key]])\n",
    "    data = pd.concat(data.values(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>MOM_2</th>\n",
       "      <th>MOM_4</th>\n",
       "      <th>MOM_6</th>\n",
       "      <th>SMA_30</th>\n",
       "      <th>SMA_60</th>\n",
       "      <th>SMA_90</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>cmf_20</th>\n",
       "      <th>ad</th>\n",
       "      <th>%K_14_3_3</th>\n",
       "      <th>%D_14_3_3</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-06-04</th>\n",
       "      <td>-1.182193</td>\n",
       "      <td>-1.182052</td>\n",
       "      <td>-1.179806</td>\n",
       "      <td>-1.181625</td>\n",
       "      <td>-1.183027</td>\n",
       "      <td>-1.041820</td>\n",
       "      <td>-0.074538</td>\n",
       "      <td>-0.097093</td>\n",
       "      <td>-0.102152</td>\n",
       "      <td>-1.175810</td>\n",
       "      <td>-1.163081</td>\n",
       "      <td>-1.154835</td>\n",
       "      <td>0.039569</td>\n",
       "      <td>0.581578</td>\n",
       "      <td>-1.422279</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.772993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-05</th>\n",
       "      <td>-1.181076</td>\n",
       "      <td>-1.182052</td>\n",
       "      <td>-1.179806</td>\n",
       "      <td>-1.180731</td>\n",
       "      <td>-1.182172</td>\n",
       "      <td>-0.948756</td>\n",
       "      <td>-0.048438</td>\n",
       "      <td>-0.093727</td>\n",
       "      <td>-0.109434</td>\n",
       "      <td>-1.175859</td>\n",
       "      <td>-1.162949</td>\n",
       "      <td>-1.154382</td>\n",
       "      <td>0.160668</td>\n",
       "      <td>0.663325</td>\n",
       "      <td>-1.417175</td>\n",
       "      <td>0.622303</td>\n",
       "      <td>0.682274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-06</th>\n",
       "      <td>-1.179512</td>\n",
       "      <td>-1.180074</td>\n",
       "      <td>-1.177760</td>\n",
       "      <td>-1.179949</td>\n",
       "      <td>-1.181425</td>\n",
       "      <td>-1.042444</td>\n",
       "      <td>-0.040282</td>\n",
       "      <td>-0.079140</td>\n",
       "      <td>-0.100331</td>\n",
       "      <td>-1.175947</td>\n",
       "      <td>-1.162813</td>\n",
       "      <td>-1.153939</td>\n",
       "      <td>0.267063</td>\n",
       "      <td>0.505318</td>\n",
       "      <td>-1.422402</td>\n",
       "      <td>0.724238</td>\n",
       "      <td>0.671269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-09</th>\n",
       "      <td>-1.179959</td>\n",
       "      <td>-1.179744</td>\n",
       "      <td>-1.178896</td>\n",
       "      <td>-1.179726</td>\n",
       "      <td>-1.181211</td>\n",
       "      <td>-0.749613</td>\n",
       "      <td>-0.050069</td>\n",
       "      <td>-0.067920</td>\n",
       "      <td>-0.103062</td>\n",
       "      <td>-1.175825</td>\n",
       "      <td>-1.162562</td>\n",
       "      <td>-1.153474</td>\n",
       "      <td>0.298405</td>\n",
       "      <td>-0.030103</td>\n",
       "      <td>-1.422823</td>\n",
       "      <td>0.792657</td>\n",
       "      <td>0.733761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-10</th>\n",
       "      <td>-1.180294</td>\n",
       "      <td>-1.179634</td>\n",
       "      <td>-1.178669</td>\n",
       "      <td>-1.178609</td>\n",
       "      <td>-1.180143</td>\n",
       "      <td>-0.850771</td>\n",
       "      <td>-0.045176</td>\n",
       "      <td>-0.058943</td>\n",
       "      <td>-0.088498</td>\n",
       "      <td>-1.175636</td>\n",
       "      <td>-1.162307</td>\n",
       "      <td>-1.153030</td>\n",
       "      <td>0.456965</td>\n",
       "      <td>-0.264233</td>\n",
       "      <td>-1.416730</td>\n",
       "      <td>0.895476</td>\n",
       "      <td>0.827491</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close Adj Close    Volume  \\\n",
       "                NVDA      NVDA      NVDA      NVDA      NVDA      NVDA   \n",
       "Date                                                                     \n",
       "2014-06-04 -1.182193 -1.182052 -1.179806 -1.181625 -1.183027 -1.041820   \n",
       "2014-06-05 -1.181076 -1.182052 -1.179806 -1.180731 -1.182172 -0.948756   \n",
       "2014-06-06 -1.179512 -1.180074 -1.177760 -1.179949 -1.181425 -1.042444   \n",
       "2014-06-09 -1.179959 -1.179744 -1.178896 -1.179726 -1.181211 -0.749613   \n",
       "2014-06-10 -1.180294 -1.179634 -1.178669 -1.178609 -1.180143 -0.850771   \n",
       "\n",
       "               MOM_2     MOM_4     MOM_6    SMA_30    SMA_60    SMA_90  \\\n",
       "                NVDA      NVDA      NVDA      NVDA      NVDA      NVDA   \n",
       "Date                                                                     \n",
       "2014-06-04 -0.074538 -0.097093 -0.102152 -1.175810 -1.163081 -1.154835   \n",
       "2014-06-05 -0.048438 -0.093727 -0.109434 -1.175859 -1.162949 -1.154382   \n",
       "2014-06-06 -0.040282 -0.079140 -0.100331 -1.175947 -1.162813 -1.153939   \n",
       "2014-06-09 -0.050069 -0.067920 -0.103062 -1.175825 -1.162562 -1.153474   \n",
       "2014-06-10 -0.045176 -0.058943 -0.088498 -1.175636 -1.162307 -1.153030   \n",
       "\n",
       "              rsi_14    cmf_20        ad %K_14_3_3 %D_14_3_3 Target  \n",
       "                NVDA      NVDA      NVDA      NVDA      NVDA   NVDA  \n",
       "Date                                                                 \n",
       "2014-06-04  0.039569  0.581578 -1.422279  0.610526  0.772993      1  \n",
       "2014-06-05  0.160668  0.663325 -1.417175  0.622303  0.682274      1  \n",
       "2014-06-06  0.267063  0.505318 -1.422402  0.724238  0.671269      1  \n",
       "2014-06-09  0.298405 -0.030103 -1.422823  0.792657  0.733761      1  \n",
       "2014-06-10  0.456965 -0.264233 -1.416730  0.895476  0.827491      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 5, 60, 8)          664       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 58, 8)          968       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 1, 29, 8)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 27, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 1, 13, 8)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 104)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 104)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1937 (7.57 KB)\n",
      "Trainable params: 1937 (7.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DJI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cp3d-\u001b[39m\u001b[38;5;132;01m{epoch}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{val_f1macro:.2f}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     ModelCheckpoint(checkpoint_path,\n\u001b[1;32m     19\u001b[0m                     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1macro\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m ]\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatagen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDJI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatagen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDJI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mdatagen\u001b[0;34m(data, seq_len, batch_size, target_index, targetcol, kind)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,ticker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tickers):\n\u001b[1;32m     30\u001b[0m         X[i] \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mxs(ticker, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 31\u001b[0m     batch\u001b[38;5;241m.\u001b[39mappend([X, \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtargetcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_index\u001b[49m\u001b[43m]\u001b[49m[t]])\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# if we get enough for a batch, dispatch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DJI'"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_len = 60\n",
    "batch_size = 128\n",
    "n_epochs = 3 # 20\n",
    "n_features = 82\n",
    "n_stocks = 5\n",
    "\n",
    "# Produce CNNpred as a binary classification problem\n",
    "model = cnnpred_3d(seq_len, n_stocks, n_features)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
    "model.summary() # print model structure to console\n",
    "\n",
    "# Set up callbacks and fit the model\n",
    "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
    "# Using the \".keras\" extension since the \".h5\" causes and error.\n",
    "# https://keras.io/api/callbacks/model_checkpoint/\n",
    "checkpoint_path = \"./cp3d-{epoch}-{val_f1macro:.2f}.keras\"\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode=\"max\",\n",
    "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
    "]\n",
    "\n",
    "model.fit(datagen(data, seq_len, batch_size, \"DJI\", \"Target\", \"train\"),\n",
    "          validation_data=datagen(data, seq_len, batch_size, \"DJI\", \"Target\", \"valid\"),\n",
    "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    # Prepare test data\n",
    "    test_data, test_target = testgen(data, seq_len, \"DJI\", \"Target\")\n",
    "\n",
    "    # Test the model\n",
    "    test_out = model.predict(test_data)\n",
    "    test_pred = (test_out > 0.5).astype(int)\n",
    "    print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "    print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "    print(\"F1:\", f1_score(test_pred, test_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNPred2\n",
    "\n",
    "This is the implementation for CNNPred2 which uses only one stock and associated features as a time series as input to the model.  The paper uses a 60/20/20 split for the training, validation and evaluation (test) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation\n",
    "Data is generated by randomly selecting a single stock for testing.  The evaluation is performed across all the datasets however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
    "    \"As a generator to produce samples for Keras model\"\n",
    "    batch = []\n",
    "    while True:\n",
    "        # Pick one dataframe from the pool\n",
    "        key = random.choice(list(data.keys()))\n",
    "        df = data[key]\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
    "        if kind == 'train':\n",
    "            index = index[:split]   # range for the training set\n",
    "        elif kind == 'valid':\n",
    "            index = index[split:]   # range for the validation set\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # Pick one position, then clip a sequence length\n",
    "        while True:\n",
    "            t = random.choice(index)     # pick one time step\n",
    "            n = (df.index == t).argmax() # find its position in the dataframe\n",
    "            if n-seq_len+1 < 0:\n",
    "                continue # this sample is not enough for one sequence length\n",
    "            frame = df.iloc[n-seq_len+1:n+1]\n",
    "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
    "            break\n",
    "        # if we get enough for a batch, dispatch\n",
    "        if len(batch) == batch_size:\n",
    "            X, y = zip(*batch)\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "            yield X, y\n",
    "            batch = []\n",
    "\n",
    "def testgen(data, seq_len, targetcol):\n",
    "    \"Return array of all test samples\"\n",
    "    batch = []\n",
    "    for key, df in data.items():\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        # find the start of test sample\n",
    "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
    "        print(t)\n",
    "        n = (df.index == t).argmax()\n",
    "        # extract sample using a sliding window\n",
    "        for i in range(n+1, len(df)+1):\n",
    "            frame = df.iloc[i-seq_len:i]\n",
    "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
    "    X, y = zip(*batch)\n",
    "    return np.expand_dims(np.array(X),3), np.array(y)\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    # Read data into pandas DataFrames.  This creates a dictionary of dataframes, one for each ticker. \n",
    "    # Each ticker is represented by a file in the datadir. \n",
    "    data = {}\n",
    "    for filename in os.listdir(DATADIR):\n",
    "        if not filename.lower().endswith(\".csv\"):\n",
    "            continue # read only the CSV files\n",
    "        filepath = os.path.join(DATADIR, filename)\n",
    "        X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
    "        # basic preprocessing: get the name, the classification\n",
    "        # Save the target variable as a column in dataframe for easier dropna()\n",
    "        name = X[\"Name\"][0]\n",
    "        del X[\"Name\"]\n",
    "        cols = X.columns\n",
    "        X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
    "        X.dropna(inplace=True)\n",
    "        # Fit the standard scaler using the training dataset\n",
    "        index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
    "        index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "        scaler = StandardScaler().fit(X.loc[index, cols])\n",
    "        # Save scale transformed dataframe\n",
    "        X[cols] = scaler.transform(X[cols])\n",
    "        data[name] = X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['NVDA'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['NVDA'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['NVDA'].head()\n",
    "#data['DJI'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 60\n",
    "batch_size = 128\n",
    "n_epochs = 3\n",
    "n_features =17 # Per dataset (old 82)\n",
    "\n",
    "# Produce CNNpred as a binary classification problem\n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
    "model.summary()  # print model structure to console\n",
    "\n",
    "# Set up callbacks and fit the model\n",
    "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
    "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.keras\"\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode=\"max\",\n",
    "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
    "]\n",
    "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
    "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
    "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    # Prepare test data\n",
    "    test_data, test_target = testgen(data, seq_len, \"Target\")\n",
    "\n",
    "    # Test the model\n",
    "    test_out = model.predict(test_data)\n",
    "    test_pred = (test_out > 0.5).astype(int)\n",
    "    print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "    print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "    print(\"F1:\", f1_score(test_pred, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    test_data, test_target = testgen(data, seq_len, \"Target\")\n",
    "    print(type(test_data))\n",
    "    print(type(data))\n",
    "    print(data['NVDA'].shape)\n",
    "    print(data['NVDA'].head(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
